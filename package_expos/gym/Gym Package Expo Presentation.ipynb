{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----Presentation-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# !pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments to choose from:\n",
    "from gym import envs\n",
    "for spec in envs.registry.all(): print(spec.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Set-up\n",
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(1000): env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Dimensions:\n",
    "print('State Space: ', env.observation_space)\n",
    "print('Action Space: ', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (2,) in the state space means that this is a two-dimensional box.\n",
    "# The Discrete(3) means that the action space consists of \n",
    "# three discrete actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give us further information about the state vector values:\n",
    "print(\"The cart position is limited to a range of [\",env.observation_space.low[0],', ',env.observation_space.high[0],'].')\n",
    "print(\"The cart's velocity is limited to a range of [\",env.observation_space.low[1],', ',env.observation_space.high[1],'].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the initial state of the environment?\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the effect of taking an action using the step() method.\n",
    "print(env.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output is the equivalence of array([A,B],C,D,{E})\n",
    "# A is the first observation state value\n",
    "# B is the second observation state value\n",
    "# C is the Reward\n",
    "# D is a Boolean indicator on whether the episode has terminated yet (goal reached or 200 steps elapsed)\n",
    "# E is any additional information which is not applicable to this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of actions executed on a random basis:\n",
    "import gym\n",
    "\n",
    "randenv = gym.make('MountainCar-v0')\n",
    "randenv.reset()\n",
    "for _ in range(1000):\n",
    "    randenv.render()\n",
    "    randenv.step(randenv.action_space.sample())\n",
    "randenv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Q-Learning Applied to the problem.\n",
    "# This is using a specific reenforcement Q-learning technique called Epsilon \n",
    "# Greedy Exploration Strategy with epsilon decay.\n",
    "# The details of setting up a reinforcement learning program is not important.\n",
    "# However, seeing the results of the application is.  \n",
    "# There are numerous reinforcement learning techinques that can be applied to \n",
    "# any of these environments.\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')     \n",
    "plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----Guide------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym \n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms.  Gym provides the environment; you provide the algorithm.\n",
    "\n",
    "#### Basics\n",
    "Reinforcement learning conists of two basic concepts:\n",
    "\n",
    "* *Environment* - A task or simulation that is constructed that needs interaction to solve.\n",
    "\n",
    "* *Agent* - An AI algorithm that is developed to interact with the environment.\n",
    "\n",
    "![image info](https://miro.medium.com/max/674/0*6yvI8Ul2ETKO-Ils.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustration above shows a simplified diagram of how the agent and environment interact.  The environment provides the agent its state and reward system.  The agent will then send an action to the environment.  The environment will respond with a change in the state and reward the agent based on the reward system criteria.  The agent will continue to respond and provide actions to maximize its rewards as it begins optimizing its machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "\n",
    "Below are some links to videos of some environments being simulated:\n",
    "\n",
    "[SpaceInvaders-v0](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/SpaceInvaders-v0/original.mp4)\n",
    "\n",
    "[Ant-v2](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/Ant-v2/original.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need Python 3.5+ installed to get started.  Install gym simply by using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking to create and use your own environment the appropriate start would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/openai/gym\n",
    "#cd gym\n",
    "#pip install ~e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your own environment, the following link will assist you: https://github.com/openai/gym/blob/master/docs/creating-environments.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Environments: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two are examples of default environments that are available in gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a list of all the environments available to use, there is a *registry.all()* object that can provide a list of all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More refined, readable list below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envids = [spec.id for spec in envs.registry.all()]\n",
    "for envid in sorted(envids):\n",
    "    print(envid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "envs = gym.envs.registry.all()\n",
    "print('Total envs available:',len(envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the environments require dependency on Atari and MujoCo. The commands to do such is:\n",
    "\n",
    "*from gym.envs.atari.env import AtariEnv*\n",
    "\n",
    "*from gym.envs.mujoco_env import MujocoEnv*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the agent interact with the environment in a designed and non-random way, we will need to understand the step function of the environment.  The step function returns four values:\n",
    "\n",
    "1. **Observation** (object) - an environment-specific *object* that represents the observation of the environment like a board state in a board game or pixel data from a camera.\n",
    "\n",
    "\n",
    "2. **Reward** (float) - the amount of reward gratned from the previous action set by the rules of the environment.  Always looks to maximize the reward.\n",
    "\n",
    "\n",
    "3. **Done** (boolean) - This determines the appropriate time to *reset* the environment again.  This gives the definitive end of the episode for the agent to take another action. Exxample: lost a life, pole tipped too far, and so on.\n",
    "\n",
    "\n",
    "4. **Info** (dict) - Provides the diagnostic information that can be useful to debug the solution but the agent cannot use or evaluate this information. \n",
    "\n",
    "\n",
    "By calling *reset()*, this process gets started by returning an initial observation.  By using *if done* it can be written more properly.  Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code should produce a video of the result and through the *print(\"Episode finished after {} timesteps\".format(t+1))* coding, you should also receive a long detailed output of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every environment consists of two important spaces: an **\"action_space\"** and an **\"observation_space\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of action_space:\n",
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of observation_space:\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore we can find out the bounds of the environment's observation_space by:\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Links for Further Studying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments:\n",
    "https://github.com/openai/gym/blob/master/docs/environments.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Environments:\n",
    "https://github.com/openai/gym/blob/master/docs/creating-environments.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrappers: (to transform observation and/or action space.  Can perform pre/postprocessing on the data that is exchanged between the agent and the environment.)\n",
    "https://github.com/openai/gym/blob/master/docs/wrappers.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents: (Facilitates the running of the algorithm against an environment.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-your-own-reinforcement-learning-environment-beb12f4151ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lastly, What is possible with OpenAI Gym?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you could teach a robot hand to solve a rubik's cube?!\n",
    "Check out the link below:\n",
    "\n",
    "https://www.youtube.com/watch?v=kVmp0uGtShk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
